{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4143efa2-65c4-4694-98a4-3bd56b403eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff972472-2069-4b54-b4b9-ca8fae8cd6d6",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e624bb2-dc73-4319-b686-e0fe25d6e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Metrics\n",
      "MSE: 31486.167775794882\n",
      "R^2: 0.7265334318706018\n",
      "\n",
      "Linear Regression Coefficients:\n",
      "bedrooms : -12.52196186860659\n",
      "bathrooms : 18.527632513048715\n",
      "sqft_living : 56.748836801162035\n",
      "sqft_lot : 10.881868446111161\n",
      "floors : 8.043720836862803\n",
      "waterfront : 63.7428995598697\n",
      "view : 48.20010852419161\n",
      "condition : 12.964269364046077\n",
      "grade : 92.23147482243314\n",
      "sqft_above : 48.29008886178811\n",
      "sqft_basement : 27.137032467668593\n",
      "yr_built : -67.64311741342638\n",
      "yr_renovated : 17.27137953034478\n",
      "lat : 78.37573693207808\n",
      "long : -1.0352030837350685\n",
      "sqft_living15 : 45.57765781263847\n",
      "sqft_lot15 : -12.930090977794954\n",
      "\n",
      "Intercept: 520.414834000001\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Ignoring excluded columns\n",
    "cols_to_drop = [\"id\", \"date\", \"zipcode\", \"Unnamed: 0\"]\n",
    "train_df = train_raw.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Splitting features and target\n",
    "X_train = train_df.drop(columns=[\"price\"])\n",
    "y_train = train_df[\"price\"] / 1000\n",
    "\n",
    "# Scaling features (fit on training only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Training model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicting on training data\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "# Computing training metrics\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(\"Training Set Metrics\")\n",
    "print(\"MSE:\", mse_train)\n",
    "print(\"R^2:\", r2_train)\n",
    "\n",
    "# Printing coefficients\n",
    "print(\"\\nLinear Regression Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, model.coef_):\n",
    "    print(feature, \":\", coef)\n",
    "\n",
    "print(\"\\nIntercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c75165-1f01-4e4f-939c-10fad8a33554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Set Metrics\n",
      "MSE: 57628.15470567037\n",
      "R^2: 0.6543560876120955\n"
     ]
    }
   ],
   "source": [
    "# Reading test data\n",
    "test_raw = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Ignorning excluded columns\n",
    "test_df = test_raw.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Splitting features and target\n",
    "X_test = test_df.drop(columns=[\"price\"])\n",
    "y_test = test_df[\"price\"] / 1000\n",
    "\n",
    "# Scaling test features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predicting on test data\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Computing testing metrics\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nTesting Set Metrics\")\n",
    "print(\"MSE:\", mse_test)\n",
    "print(\"R^2:\", r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6e89b-3b62-493e-891c-f86a5a37ed82",
   "metadata": {},
   "source": [
    "### Which features contribute mostly to the linear regression model?\n",
    "Since all of the features are standardized, the coefficients are directly comparable across features. The features that contribute the most to the linear regression model include grade (92.2), lat (78.4), year_built(-67.6), waterfront (63.7), and sqft_living (56.7). All of these features make sense as the quality of the house has a major impact on price, location and the year the house was built plays a significant role in determining house value, and waterfront view and sqft_living strongly affect the house price.\n",
    "### Is the model fitting the data well?\n",
    "The training R-squared (0.73) indicates that about 73% of the variance in house prices is explained by the model, and the testing R-squared (0.65) shows that the model explains 65% of price variation. The model fits reasonably well for a simple linear regression, but there is room for improvement.\n",
    "### How large is the model error?\n",
    "There is some overfitting in the model, but not extreme overfitting, because the R-squared only drops from 0.73 to 0.65. The model fits the training data better than the testing data. The model generalizes well, but is still off by a substantial amount for individual prediction error.\n",
    "### How do the training and testing MSE relate?\n",
    "The testing MSE is higher than the training MSE, which is expected because the model was optimized on the training data. The increase means some loss of performance on unseen data, which means mild overfitting. However, since the difference is not extreme and the R-squared remains relatively high on the test set, the model still performs reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada1064-6fb2-423b-8aa0-6c6aaa4690f6",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e4e79b-8457-4c5e-8594-dd41832288c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics\n",
      "MSE: 31486.16777579488\n",
      "R^2: 0.7265334318706018\n",
      "\n",
      "Testing Set Metrics\n",
      "MSE: 57628.1547056704\n",
      "R^2: 0.6543560876120953\n",
      "\n",
      "Closed-Form Intercept: 520.4148340000008\n",
      "\n",
      "Closed-Form Coefficients:\n",
      "bedrooms : -12.521961868606303\n",
      "bathrooms : 18.52763251304865\n",
      "sqft_living : 56.748836801162014\n",
      "sqft_lot : 10.881868446111227\n",
      "floors : 8.04372083686274\n",
      "waterfront : 63.74289955986932\n",
      "view : 48.20010852419148\n",
      "condition : 12.964269364046025\n",
      "grade : 92.23147482243324\n",
      "sqft_above : 48.29008886178805\n",
      "sqft_basement : 27.137032467668448\n",
      "yr_built : -67.64311741342641\n",
      "yr_renovated : 17.271379530344895\n",
      "lat : 78.37573693207827\n",
      "long : -1.0352030837349417\n",
      "sqft_living15 : 45.577657812638385\n",
      "sqft_lot15 : -12.930090977794892\n"
     ]
    }
   ],
   "source": [
    "# Converting scaled training data to numpy\n",
    "X_train_np = X_train_scaled\n",
    "y_train_np = y_train.to_numpy(dtype=float)\n",
    "\n",
    "X_test_np = X_test_scaled\n",
    "y_test_np = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Adding bias term\n",
    "X_train_b = np.c_[np.ones((X_train_np.shape[0], 1)), X_train_np]\n",
    "X_test_b  = np.c_[np.ones((X_test_np.shape[0], 1)), X_test_np]\n",
    "\n",
    "# Closed-form solution\n",
    "theta = np.linalg.pinv(X_train_b) @ y_train_np\n",
    "\n",
    "# Function to predict a testing point\n",
    "def predict_point(x_new, theta):\n",
    "    x_new = np.array(x_new, dtype=float).reshape(1, -1)\n",
    "    x_new_b = np.c_[np.ones((1, 1)), x_new]\n",
    "    return float(x_new_b @ theta)\n",
    "\n",
    "# Predicting values\n",
    "y_train_pred_cf = X_train_b @ theta\n",
    "y_test_pred_cf  = X_test_b @ theta\n",
    "\n",
    "# Calculating and printing metrics\n",
    "mse_train_cf = mean_squared_error(y_train_np, y_train_pred_cf)\n",
    "r2_train_cf  = r2_score(y_train_np, y_train_pred_cf)\n",
    "\n",
    "mse_test_cf = mean_squared_error(y_test_np, y_test_pred_cf)\n",
    "r2_test_cf  = r2_score(y_test_np, y_test_pred_cf)\n",
    "\n",
    "print(\"\\nTraining Set Metrics\")\n",
    "print(\"MSE:\", mse_train_cf)\n",
    "print(\"R^2:\", r2_train_cf)\n",
    "\n",
    "print(\"\\nTesting Set Metrics\")\n",
    "print(\"MSE:\", mse_test_cf)\n",
    "print(\"R^2:\", r2_test_cf)\n",
    "\n",
    "# Print intercept and coefficients\n",
    "print(\"\\nClosed-Form Intercept:\", theta[0])\n",
    "\n",
    "print(\"\\nClosed-Form Coefficients:\")\n",
    "for feature, coef in zip(X_train.columns, theta[1:]):\n",
    "    print(feature, \":\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ec37f6-4156-43f9-9983-8058c7c319ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with sklearn LinearRegression\n",
      "\n",
      "Training Set\n",
      "Sklearn MSE: 31486.167775794882\n",
      "Closed-Form MSE: 31486.16777579488\n",
      "Sklearn R^2: 0.7265334318706018\n",
      "Closed-Form R^2: 0.7265334318706018\n",
      "\n",
      "Testing Set\n",
      "Sklearn MSE: 57628.15470567037\n",
      "Closed-Form MSE: 57628.1547056704\n",
      "Sklearn R^2: 0.6543560876120955\n",
      "Closed-Form R^2: 0.6543560876120953\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison with sklearn LinearRegression\")\n",
    "\n",
    "print(\"\\nTraining Set\")\n",
    "print(\"Sklearn MSE:\", mse_train)\n",
    "print(\"Closed-Form MSE:\", mse_train_cf)\n",
    "\n",
    "print(\"Sklearn R^2:\", r2_train)\n",
    "print(\"Closed-Form R^2:\", r2_train_cf)\n",
    "\n",
    "print(\"\\nTesting Set\")\n",
    "print(\"Sklearn MSE:\", mse_test)\n",
    "print(\"Closed-Form MSE:\", mse_test_cf)\n",
    "\n",
    "print(\"Sklearn R^2:\", r2_test)\n",
    "print(\"Closed-Form R^2:\", r2_test_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d6d0a-c59a-43fb-8f04-5a5c92bc0b4b",
   "metadata": {},
   "source": [
    "The closed-form model produces the same MSE and R² values as the sklearn LinearRegression model on both the training and testing sets. This confirms that the closed-form solution correctly uses the ordinary least squares regression formula. Since sklearn’s LinearRegression also has the same output, the implementation is equivalent to the package solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcb245-eda9-4813-8264-9f8d62ab0c5e",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0698745-7846-4933-ae55-359ef830804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building polynomial features\n",
    "def make_poly_features(X_np, p):\n",
    "    X_np = np.array(X_np, dtype=float)\n",
    "    \n",
    "    # Creating list\n",
    "    features = [X_np ** deg for deg in range(1, p + 1)]\n",
    "    \n",
    "    # Concatenating column-wise\n",
    "    return np.concatenate(features, axis=1)\n",
    "\n",
    "# Fitting polynomial regression with closed-form solution\n",
    "def fit_poly_regression(X_train_np, y_train_np, p):\n",
    "    \n",
    "    # Building polynomial de/sign matrix\n",
    "    X_poly = make_poly_features(X_train_np, p)\n",
    "    \n",
    "    # Adding bias column of ones\n",
    "    X_b = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    \n",
    "    # Closed-form least squares solution\n",
    "    theta = np.linalg.pinv(X_b) @ y_train_np\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Predicting using learned theta\n",
    "def predict_poly(X_np, theta, p):\n",
    "    \n",
    "    # Building polynomial features\n",
    "    X_poly = make_poly_features(X_np, p)\n",
    "    \n",
    "    # Adding bias\n",
    "    X_b = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "    \n",
    "    # Returning predictions\n",
    "    return X_b @ theta\n",
    "\n",
    "# Evaluating model\n",
    "def evaluate_poly_model(X_train_np, y_train_np, X_test_np, y_test_np, p):\n",
    "    \n",
    "    # Fitting model\n",
    "    theta = fit_poly_regression(X_train_np, y_train_np, p)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = predict_poly(X_train_np, theta, p)\n",
    "    y_test_pred  = predict_poly(X_test_np, theta, p)\n",
    "    \n",
    "    # Metrics\n",
    "    mse_train = mean_squared_error(y_train_np, y_train_pred)\n",
    "    r2_train  = r2_score(y_train_np, y_train_pred)\n",
    "    \n",
    "    mse_test = mean_squared_error(y_test_np, y_test_pred)\n",
    "    r2_test  = r2_score(y_test_np, y_test_pred)\n",
    "    \n",
    "    return theta, mse_train, r2_train, mse_test, r2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee524a3f-6c8c-4611-bff8-a399cdce0d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial Regression Results using X = sqft_living (p <= 5)\n",
      " p    MSE_train  R2_train      MSE_test   R2_test\n",
      " 1 57947.526161  0.496709  88575.978543  0.468736\n",
      " 2 54822.665116  0.523849  71791.679479  0.569406\n",
      " 3 53785.194716  0.532860  99833.483763  0.401216\n",
      " 4 52795.774758  0.541453 250979.274285 -0.505331\n",
      " 5 52626.111955  0.542927 570616.914821 -2.422464\n"
     ]
    }
   ],
   "source": [
    "feature = \"sqft_living\"\n",
    "col_idx = list(X_train.columns).index(feature)\n",
    "\n",
    "# Keeping X as (N, 1)\n",
    "X_train_np = np.array(X_train_scaled[:, [col_idx]], dtype=float)\n",
    "X_test_np  = np.array(X_test_scaled[:,  [col_idx]], dtype=float)\n",
    "\n",
    "# y as numpy\n",
    "y_train_np = y_train.to_numpy(dtype=float)\n",
    "y_test_np  = y_test.to_numpy(dtype=float)\n",
    "\n",
    "# Training models for p <= 5 and calculating metrics\n",
    "results = []\n",
    "\n",
    "for p in range(1, 6):\n",
    "    theta, mse_tr, r2_tr, mse_te, r2_te = evaluate_poly_model(\n",
    "        X_train_np, y_train_np, X_test_np, y_test_np, p\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"p\": p,\n",
    "        \"MSE_train\": mse_tr,\n",
    "        \"R2_train\": r2_tr,\n",
    "        \"MSE_test\": mse_te,\n",
    "        \"R2_test\": r2_te\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Printing results\n",
    "print(\"\\nPolynomial Regression Results using X = sqft_living (p <= 5)\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268597b-0061-46c0-a457-86872e6c68bb",
   "metadata": {},
   "source": [
    "As the polynomial degree increases, the training MSE gets smaller and the training R-squared gets larger. This happens because more complex models fit the training data more closely. On the testing data, performance improves from degree 1 to degree 2. The test MSE decreases and the test R-squared increases, meaning the model predicts better. However, after degree 2, the test MSE increases sharply and the test R-squared decreases, becoming negative for higher degrees. This shows that higher-degree models are overfitting. They fit the training data well but perform poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c1d1d-2704-4006-abe2-9e372bc62be8",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb1b35b2-754a-4045-8ea6-da214e6a82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core gradient descent function\n",
    "def gd_train_linear_regression(X, y, alpha, num_iters, theta0=None, return_history=False):\n",
    "    \n",
    "    m, n = X.shape\n",
    "\n",
    "    # initializing theta\n",
    "    theta = np.zeros(n) if theta0 is None else theta0.copy()\n",
    "\n",
    "    cost_history = [] if return_history else None\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "\n",
    "        # predicting\n",
    "        preds = X @ theta\n",
    "\n",
    "        # errors\n",
    "        errors = preds - y\n",
    "\n",
    "        # gradient\n",
    "        gradient = (2.0 / m) * (X.T @ errors)\n",
    "\n",
    "        # parameter update\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    if return_history:\n",
    "        return theta, cost_history\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_theta(X_train_b, y_train, X_test_b, y_test, theta):\n",
    "\n",
    "    y_train_pred = X_train_b @ theta\n",
    "    y_test_pred  = X_test_b  @ theta\n",
    "\n",
    "    metrics = {\n",
    "        \"mse_train\": mean_squared_error(y_train, y_train_pred),\n",
    "        \"r2_train\":  r2_score(y_train, y_train_pred),\n",
    "        \"mse_test\":  mean_squared_error(y_test, y_test_pred),\n",
    "        \"r2_test\":   r2_score(y_test, y_test_pred)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Wrapper function\n",
    "def train_and_evaluate_gd(X_train_b, y_train, X_test_b, y_test, alpha, num_iters, theta0=None):\n",
    "\n",
    "    theta = gd_train_linear_regression(\n",
    "        X_train_b,\n",
    "        y_train,\n",
    "        alpha=alpha,\n",
    "        num_iters=num_iters,\n",
    "        theta0=theta0\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_theta(X_train_b, y_train, X_test_b, y_test, theta)\n",
    "\n",
    "    return theta, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10df94dc-a020-4f38-b538-0be18dab38ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate (alpha): 0.01\n",
      "Iterations: 10\n",
      "Theta0 (Intercept): 95.1980\n",
      "First 5 Thetas: [11.9286 20.2833 32.1165  5.3808  9.5255]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.01\n",
      "Iterations: 50\n",
      "Theta0 (Intercept): 330.8955\n",
      "First 5 Thetas: [ 6.0053 23.8214 54.6659  3.6375 11.122 ]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.01\n",
      "Iterations: 100\n",
      "Theta0 (Intercept): 451.3976\n",
      "First 5 Thetas: [-3.6569 19.2418 56.955   2.6937 10.4482]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.1\n",
      "Iterations: 10\n",
      "Theta0 (Intercept): 464.5357\n",
      "First 5 Thetas: [-4.6456 18.7334 57.1255  2.53   10.5525]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.1\n",
      "Iterations: 50\n",
      "Theta0 (Intercept): 520.4074\n",
      "First 5 Thetas: [-12.4572  17.6212  57.1878   7.8533   8.006 ]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.1\n",
      "Iterations: 100\n",
      "Theta0 (Intercept): 520.4148\n",
      "First 5 Thetas: [-12.5191  18.4242  56.7696  10.2707   8.061 ]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.5\n",
      "Iterations: 10\n",
      "Theta0 (Intercept): 520.4148\n",
      "First 5 Thetas: [-40545159.6361 -60243904.6471 -66812541.2293 -25606793.6371\n",
      " -37406765.8138]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.5\n",
      "Iterations: 50\n",
      "Theta0 (Intercept): 2259458857778344448.0000\n",
      "First 5 Thetas: [-3.7710e+32 -5.6031e+32 -6.2140e+32 -2.3816e+32 -3.4791e+32]\n",
      "\n",
      "\n",
      "Learning Rate (alpha): 0.5\n",
      "Iterations: 100\n",
      "Theta0 (Intercept): 36336718093549767234830254743525414790925450739712.0000\n",
      "First 5 Thetas: [-6.1249e+63 -9.1007e+63 -1.0093e+64 -3.8683e+63 -5.6508e+63]\n",
      "\n",
      "\n",
      "Metrics Table\n",
      " alpha  num_iters   mse_train     r2_train    mse_test      r2_test\n",
      "  0.01         10  2.3573e+05  -1.0474e+00  2.8057e+05  -6.8280e-01\n",
      "  0.01         50  6.9720e+04   3.9446e-01  9.7050e+04   4.1791e-01\n",
      "  0.01        100  3.6820e+04   6.8020e-01  6.3333e+04   6.2014e-01\n",
      "  0.10         10  3.5105e+04   6.9510e-01  6.1630e+04   6.3035e-01\n",
      "  0.10         50  3.1497e+04   7.2644e-01  5.7722e+04   6.5379e-01\n",
      "  0.10        100  3.1486e+04   7.2653e-01  5.7639e+04   6.5429e-01\n",
      "  0.50         10  1.4561e+17  -1.2646e+12  1.6261e+17  -9.7529e+11\n",
      "  0.50         50  1.2595e+67  -1.0939e+62  1.4066e+67  -8.4366e+61\n",
      "  0.50        100 3.3228e+129 -2.8859e+124 3.7107e+129 -2.2256e+124\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01, 0.1, 0.5]\n",
    "iters_list = [10, 50, 100]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    for num_iters in iters_list:\n",
    "\n",
    "        # Training model\n",
    "        theta = gd_train_linear_regression(\n",
    "            X_train_b,\n",
    "            y_train_np,\n",
    "            alpha=alpha,\n",
    "            num_iters=num_iters\n",
    "        )\n",
    "\n",
    "        # Evaluating metrics\n",
    "        metrics = evaluate_theta(\n",
    "            X_train_b, y_train_np,\n",
    "            X_test_b,  y_test_np,\n",
    "            theta\n",
    "        )\n",
    "\n",
    "        # Printing theta (separating theta0)\n",
    "        np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "        theta0 = theta[0]\n",
    "        first_five = theta[1:6]   # first 5 coefficients after intercept\n",
    "\n",
    "        print(\"Learning Rate (alpha):\", alpha)\n",
    "        print(\"Iterations:\", num_iters)\n",
    "        print(f\"Theta0 (Intercept): {theta0:.4f}\")\n",
    "        print(\"First 5 Thetas:\", np.round(first_five, 4))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Storing metrics for table\n",
    "        rows.append([\n",
    "            alpha,\n",
    "            num_iters,\n",
    "            metrics[\"mse_train\"],\n",
    "            metrics[\"r2_train\"],\n",
    "            metrics[\"mse_test\"],\n",
    "            metrics[\"r2_test\"]\n",
    "        ])\n",
    "\n",
    "# Creating results table\n",
    "results_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\n",
    "        \"alpha\",\n",
    "        \"num_iters\",\n",
    "        \"mse_train\",\n",
    "        \"r2_train\",\n",
    "        \"mse_test\",\n",
    "        \"r2_test\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Metrics Table\")\n",
    "clean_df = results_df.copy()\n",
    "\n",
    "# Formatting numeric columns\n",
    "for col in clean_df.columns:\n",
    "    if col not in [\"alpha\", \"num_iters\"]:\n",
    "        clean_df[col] = clean_df[col].map(lambda x: f\"{x:.4e}\")\n",
    "\n",
    "print(clean_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e976217-841f-4e4d-9d89-248877f3743b",
   "metadata": {},
   "source": [
    "For α = 0.01, the algorithm improves as the number of iterations increases. The MSE decreases and R² increases from negative values to positive values by 100 iterations. For α = 0.1, the algorithm converges much faster and achieves the best performance. By 50 iterations, both training and testing R-squared values are already around 0.7 and stabilize by 100 iterations. The MSE also plateaus, showing that the algorithm reached the optimal solution within about 50–100 iterations. For α = 0.5, the algorithm diverges. MSE becomes extremely large, and R-squared becomes highly negative. This shows that the learning rate is too large. In conclusion, the algorithm converges when the learning rate is appropriately chosen (a = 0.1), requires around 50–100 iterations to stabilize, and fails to converge when the learning rate is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed8538-e355-4dae-b745-3ebf8594f92b",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b052f066-592c-4111-8e1d-6812d6f6b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_train_ridge_regression(X, y, alpha, num_iters, lam, theta0=None):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n) if theta0 is None else theta0.astype(float).copy()\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        preds = X @ theta\n",
    "        errors = preds - y\n",
    "\n",
    "        # Fitting the gradient\n",
    "        gradient = (2.0 / m) * (X.T @ errors)\n",
    "\n",
    "        # Ridge gradient scaled by m\n",
    "        ridge_term = (2.0 * lam / m) * theta\n",
    "        ridge_term[0] = 0.0\n",
    "\n",
    "        gradient += ridge_term\n",
    "\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb3ca642-9396-4839-871c-592346b4ce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear (lambda=0)\n",
      "  intercept: 1.1377269735725186\n",
      "  slope: 1.94527518078825\n",
      "  MSE: 1.9499357635589158\n",
      "  R^2: 0.725823805647487\n",
      "\n",
      "Ridge (lambda=1)\n",
      "  intercept: 1.1376714374282075\n",
      "  slope: 1.943850219320922\n",
      "  MSE: 1.9499385334705766\n",
      "  R^2: 0.7258234161762789\n",
      "\n",
      "Ridge (lambda=10)\n",
      "  intercept: 1.1371752494913991\n",
      "  slope: 1.9311188945363944\n",
      "  MSE: 1.95020913827749\n",
      "  R^2: 0.7257853670274199\n",
      "\n",
      "Ridge (lambda=100)\n",
      "  intercept: 1.1325488744360113\n",
      "  slope: 1.8124141074269553\n",
      "  MSE: 1.9740156919277065\n",
      "  R^2: 0.7224379796916689\n",
      "\n",
      "Ridge (lambda=1000)\n",
      "  intercept: 1.1056067050737797\n",
      "  slope: 1.1224487965504333\n",
      "  MSE: 2.873519116993794\n",
      "  R^2: 0.5959607743905316\n",
      "\n",
      "Ridge (lambda=10000)\n",
      "  intercept: 1.0709647236308166\n",
      "  slope: 0.23350905283446305\n",
      "  MSE: 5.947068166910269\n",
      "  R^2: 0.16379577828633074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Simulating data\n",
    "N = 1000\n",
    "X = np.random.uniform(-2, 2, size=N)\n",
    "\n",
    "# e_i ~ N(0, 2)  => std = sqrt(2)\n",
    "e = np.random.normal(loc=0.0, scale=np.sqrt(2), size=N)\n",
    "\n",
    "y = 1 + 2 * X + e\n",
    "\n",
    "# Designing matrix with intercept\n",
    "X_b = np.column_stack([np.ones(N), X])\n",
    "\n",
    "# Iterating\n",
    "num_iters = 5000\n",
    "\n",
    "# Preventing lamba from exploding\n",
    "def pick_alpha(lam):\n",
    "    if lam <= 10:\n",
    "        return 0.1\n",
    "    elif lam <= 100:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# Fitting linear + ridge models and printing\n",
    "lambdas = [0, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "for lam in lambdas:\n",
    "    alpha = pick_alpha(lam)\n",
    "\n",
    "    if lam == 0:\n",
    "        theta = gd_train_linear_regression(X_b, y, alpha=alpha, num_iters=num_iters)\n",
    "        model_name = \"Linear (lambda=0)\"\n",
    "    else:\n",
    "        theta = gd_train_ridge_regression(X_b, y, alpha=alpha, num_iters=num_iters, lam=lam)\n",
    "        model_name = f\"Ridge (lambda={lam})\"\n",
    "\n",
    "    # Predictions on full dataset\n",
    "    y_pred = X_b @ theta\n",
    "\n",
    "    # Preventing NaN/Inf so code doesn't crash\n",
    "    if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
    "        print(model_name)\n",
    "        print(\"  slope:\", theta[1])\n",
    "        print(\"  MSE: diverged (NaN/Inf)\")\n",
    "        print(\"  R^2: diverged (NaN/Inf)\")\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    print(model_name)\n",
    "    print(\"  intercept:\", theta[0])\n",
    "    print(\"  slope:\", theta[1])\n",
    "    print(\"  MSE:\", mse)\n",
    "    print(\"  R^2:\", r2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8f955-7bb3-4fac-9ab9-378cc87be4d9",
   "metadata": {},
   "source": [
    "When lambda = 0 (ordinary linear regression), the estimated slope is close to the true value 2, and the model achieves the lowest MSE and highest R-squared.For small values of lambda (1 and 10), the slope changes only slightly, and the performance metrics remain almost the same, indicating that mild regularization has little effect. However, as lambda increases to 100 and 100, the slope shrinks toward zero, which is caused by ridge regression. The MSE increases and R-squared decreases, which means the model underfits the data. When lambda is 10,000, the slope is close to zero, meaning the model ignores the relationship between X and Y. The MSE becomes larger and R-squared is negative, showing strong underfitting. In conclusion, as lambda increases, the model starts to underfit and coefficients shrink to zero, meaning smaller lambda helps with good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
